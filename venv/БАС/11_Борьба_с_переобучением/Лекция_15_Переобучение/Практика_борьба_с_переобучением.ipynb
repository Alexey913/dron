{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c32c70c-60b6-490e-92ce-0a2d1b1daf49",
   "metadata": {},
   "source": [
    "Практические аспекты и рекомендации при работе с переобучением"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35ed365-5386-47f8-86ec-32755ec7c7ce",
   "metadata": {},
   "source": [
    "**Переобучение** (англ. overfitting) — это феномен в машинном обучении, когда модель слишком точно соответствует обучающим данным, включая их шум и выбросы, и, как результат, демонстрирует плохую обобщающую способность на новых данных."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fac833-a2a6-4b05-82f0-bd51186cb6a1",
   "metadata": {},
   "source": [
    "## Комбинирование методов регуляризации"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721be9dc-6055-4657-b453-dbb964cd3990",
   "metadata": {},
   "source": [
    "Синергия различных методов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa20f3b-c3a6-473b-b97f-a15a773cc761",
   "metadata": {},
   "source": [
    "### Почему стоит комбинировать методы регуляризации\n",
    "\n",
    "- Многоаспектность переобучения: Переобучение может возникать по разным причинам, и использование нескольких методов регуляризации позволяет адресовать эти причины комплексно.\n",
    "- Взаимодополняемость методов: Разные методы воздействуют на различные аспекты модели — архитектуру, алгоритм обучения, данные — что способствует более эффективной борьбе с переобучением.\n",
    "- Улучшение обобщающей способности: Комбинация методов помогает модели лучше обобщать и адаптироваться к новым данным."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bfee11-2e28-41fc-80f2-e399fa454ad2",
   "metadata": {},
   "source": [
    "## Как методы дополняют друг друга\n",
    "\n",
    "- Архитектурные методы + алгоритмические методы\n",
    "\n",
    "Пример: Использование Dropout вместе с L2-регуляризацией. Dropout снижает вероятность совместного адаптирования нейронов, а L2-регуляризация контролирует величину весов, предотвращая их чрезмерный рост.\n",
    "\n",
    "- Архитектурные методы + методы изменения данных\n",
    "\n",
    "Пример: Применение Batch Normalization для стабилизации обучения в сочетании с аугментацией данных, которая увеличивает разнообразие входных данных.\n",
    "\n",
    "- Алгоритмические методы + методы изменения данных\n",
    "\n",
    "Пример: Использование L1-регуляризации для разрежения модели и балансировки классов в данных для улучшения качества на редких классах."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e68972-1b03-4ef8-ac84-4930d9a53b06",
   "metadata": {},
   "source": [
    "### Предостережения при комбинировании методов\n",
    "\n",
    "- Избегайте переусложнения модели: Использование слишком большого количества методов регуляризации может привести к недообучению, когда модель не способна уловить основные закономерности в данных.\n",
    "- Учитывайте взаимодействие методов: Некоторые методы могут конфликтовать или снижать эффективность друг друга. Например, применение Dropout и Batch Normalization требует особой осторожности, так как они могут мешать друг другу при неправильной настройке.\n",
    "- Настраивайте гиперпараметры комплексно: При комбинировании методов важно тщательно подбирать гиперпараметры каждого из них, учитывая их взаимодействие."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51226015-9370-4469-a6cb-5f4de69bce22",
   "metadata": {},
   "source": [
    "## Примеры успешных комбинаций в известных моделях\n",
    "\n",
    "### ResNet (Residual Networks)\n",
    "\n",
    "**Используемые методы:**\n",
    "- Batch Normalization: Стабилизирует и ускоряет обучение глубоких нейронных сетей.\n",
    "- Skip Connections: Помогают бороться с проблемой затухания градиента.\n",
    "- Аугментация данных: Увеличивает разнообразие обучающего набора (например, случайное обрезание, изменение яркости)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099dbf8a-ad9f-4655-940f-7f8ebad31c84",
   "metadata": {},
   "source": [
    "### Inception Networks\n",
    "**Используемые методы**\n",
    "- Архитектурные инновации: Комбинируют свертки разного размера в одном слое.\n",
    "- Batch Normalization и Dropout: Предотвращают переобучение и улучшают обобщающую способность.\n",
    "- Аугментация данных: Применение разнообразных преобразований изображений для увеличения обучающего набора."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fccf74-c674-4ad3-925b-8005eb71484d",
   "metadata": {},
   "source": [
    "## Transformer Models\n",
    "**Используемые методы**\n",
    "- Dropout: Применяется в слоях внимания и полносвязных слоях.\n",
    "- Регуляризация весов: L2-регуляризация используется для предотвращения переобучения.\n",
    "- Маскирование входных данных: Помогает модели предсказывать скрытые токены, улучшая понимание контекста."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b681e7-e2d4-4dc1-aad1-c5422bf928bb",
   "metadata": {},
   "source": [
    "## Влияние на вычислительные ресурсы и время обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b5fd28-ef03-426c-837d-8938d979c41f",
   "metadata": {},
   "source": [
    "### Увеличение вычислительных затрат\n",
    "\n",
    "**Дополнительные вычисления:**\n",
    "- Методы, такие как Batch Normalization, добавляют операции по вычислению средних и дисперсий.\n",
    "- Аугментация данных требует дополнительных ресурсов для обработки каждого батча данных на лету."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e36c3c0-ef1d-48ea-b141-5635adc4dc34",
   "metadata": {},
   "source": [
    "### Увеличение времени обучения\n",
    "**Медленнее сходимость**\n",
    "- Dropout может замедлить обучение, так как модель обучается на \"усеченных\" версиях самой себя.\n",
    "- Комбинирование методов увеличивает сложность модели, требуя больше эпох для обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45bfeb2-71dc-4e00-ad1b-5d4a400a55ad",
   "metadata": {},
   "source": [
    "## Увеличение потребления памяти\n",
    "**Дополнительные параметры**\n",
    "- Методы, такие как Batch Normalization, добавляют обучаемые параметры (гамма и бета).\n",
    "- Хранение статистик по мини-батчам требует дополнительной памяти."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142a9d36-0abc-47f8-aa12-ea072c2a11e6",
   "metadata": {},
   "source": [
    "## Практические рекомендации\n",
    "**Оптимизация кода**\n",
    "- Используйте эффективные библиотеки и фреймворки, поддерживающие ускорение на GPU/TPU.\n",
    "- Параллелизуйте операции, где это возможно."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6f52e7-32ea-468b-a901-283e1bec3306",
   "metadata": {},
   "source": [
    "**Баланс между качеством и ресурсами**\n",
    "- Оценивайте, насколько прирост качества оправдывает увеличение вычислительных затрат.\n",
    "- Возможно, стоит отказаться от некоторых методов, если они не дают существенного улучшения."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53127c9-a4b1-45f9-8852-31f9c38f3409",
   "metadata": {},
   "source": [
    "# Настройка гиперпараметров"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4810868-03b7-4459-9757-f1de3628d4b6",
   "metadata": {},
   "source": [
    "## Роль гиперпараметров в регуляризации"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02df42d-b136-40ff-97af-20ef28409180",
   "metadata": {},
   "source": [
    "**Что такое гиперпараметры**\n",
    "\n",
    "**Определение**: Параметры модели, которые устанавливаются до начала обучения и не обновляются в процессе обучения. Они контролируют процесс обучения и структуру модели."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d9666a-003e-4cb1-9ab7-3c93f001a5dc",
   "metadata": {},
   "source": [
    "**Влияние гиперпараметров на регуляризацию**\n",
    "- Коэффициент регуляризации \n",
    "𝜆\n",
    "λ: Определяет степень влияния регуляризационного члена в функции потерь.\n",
    "- Вероятность отключения нейронов в Dropout: Устанавливает долю нейронов, которые будут отключены в процессе обучения.\n",
    "- Параметры аугментации: Определяют, какие преобразования будут применяться к данным и с какой вероятностью."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6334a49f-4ab3-4160-a1d4-a08d3dd9aa9f",
   "metadata": {},
   "source": [
    "**Последствия неправильной настройки**\n",
    "- Слишком сильная регуляризация:\n",
    "Модель может недообучиться, не уловив важные зависимости в данных.\n",
    "- Слишком слабая регуляризация:\n",
    "Модель может переобучиться, плохо обобщая на новые данные."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a5634b-9e23-4607-b1e9-03a97206c99c",
   "metadata": {},
   "source": [
    "## Техники подбора гиперпараметров"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb8f503-d44b-46eb-8cef-d363fd26f68e",
   "metadata": {},
   "source": [
    "**Grid Search (Полный перебор)**\n",
    "Перебор всех возможных комбинаций заданных значений гиперпараметров"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca7256e-f2a2-4897-b9e0-0993a212bed3",
   "metadata": {},
   "source": [
    "**Преимущества:**\n",
    "- Простота реализации.\n",
    "- Гарантирует, что все комбинации будут протестированы.\n",
    "  \n",
    "**Недостатки:**\n",
    "- Экспоненциальный рост числа комбинаций при увеличении числа гиперпараметров.\n",
    "- Высокие вычислительные затраты."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e5de93-b95a-4938-815e-3d5a0f77e992",
   "metadata": {},
   "source": [
    "**Random Search (Случайный поиск)**\n",
    "\n",
    "**Описание:**\n",
    "Случайное выборочное исследование пространства гиперпараметров в заданных диапазонах.\n",
    "\n",
    "**Преимущества:**\n",
    "Эффективнее при ограниченных ресурсах.\n",
    "Может найти хорошие комбинации быстрее, чем Grid Search.\n",
    "\n",
    "**Недостатки:**\n",
    "Возможность пропустить оптимальные значения."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef44fb5-99b7-4a75-a5eb-f774876fc809",
   "metadata": {},
   "source": [
    "**Bayesian Optimization (Байесовская оптимизация)**\n",
    "\n",
    "**Описание:**\n",
    "- Строит вероятностную модель зависимости качества модели от гиперпараметров.\n",
    "- Выбирает следующие значения гиперпараметров на основе предыдущих результатов.\n",
    "\n",
    "**Преимущества:**\n",
    "- Эффективное исследование пространства гиперпараметров.\n",
    "- Быстро сходится к оптимальным значениям.\n",
    "\n",
    "**Недостатки:**\n",
    "- Более сложная реализация.\n",
    "- Требует дополнительных вычислительных ресурсов для построения модели."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32900ff9-5ed9-461f-9cba-dc4e13ccc566",
   "metadata": {},
   "source": [
    "**Hyperband и другие адаптивные методы**\n",
    "\n",
    "**Hyperband:**\n",
    "- Комбинирует случайный поиск с ранней остановкой.\n",
    "- Эффективно распределяет ресурсы между большим числом гиперпараметров."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e6f746-fe7b-4b23-af92-a6000ec8c6a0",
   "metadata": {},
   "source": [
    "**Инструменты для оптимизации гиперпараметров**\n",
    "\n",
    "- Sklearn:\n",
    "GridSearchCV, RandomizedSearchCV.\n",
    "- Optuna:\n",
    "Современная библиотека для эффективной оптимизации гиперпараметров с поддержкой Байесовской оптимизации.\n",
    "- Hyperopt:\n",
    "Поддерживает различные алгоритмы оптимизации, включая TPE (Tree-structured Parzen Estimator).\n",
    "- Keras Tuner:\n",
    "Инструмент для настройки гиперпараметров в моделях Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca03b2de-7b6c-41d9-a3a6-a2b12ab3f7da",
   "metadata": {},
   "source": [
    "**Валидационные стратегии**\n",
    "\n",
    "**Кросс-валидация**\n",
    "\n",
    "**Описание:**\n",
    "- Разделение данных на K частей (фолдов).\n",
    "- Модель обучается на K-1 фолдах и проверяется на оставшемся.\n",
    "- Процесс повторяется K раз с разными фолдами.\n",
    "  \n",
    "**Преимущества:**\n",
    "- Более надежная оценка производительности модели.\n",
    "- Использует все данные как для обучения, так и для валидации.\n",
    "  \n",
    "**Недостатки:**\n",
    "- Увеличение времени обучения в K раз.\n",
    "- Не всегда подходит для больших данных или временных рядов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e32be7a-c7c7-4765-a7f1-eef0b0b079af",
   "metadata": {},
   "source": [
    "**Отложенная выборка (Hold-out validation)**\n",
    "\n",
    "**Описание:**\n",
    "Разделение данных на обучающую и валидационную выборки один раз.\n",
    "    \n",
    "**Преимущества:**\n",
    "Быстрота и простота.\n",
    "\n",
    "**Недостатки:**\n",
    "Результаты могут зависеть от случайного разделения данных.\n",
    "Меньшая надежность оценки по сравнению с кросс-валидацией."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0487bcd-400f-454d-9e85-ac583686c83d",
   "metadata": {},
   "source": [
    "### Stratified Sampling (Стратифицированная выборка)\n",
    "\n",
    "**Описание:**\n",
    "Разделение данных с сохранением пропорций классов в обучающей и валидационной выборках.\n",
    "\n",
    "**Преимущества:**\n",
    "Обеспечивает представительность выборок.\n",
    "Важно при работе с несбалансированными данными.\n",
    "\n",
    "**Применение:**\n",
    "В задачах классификации, где классы представлены неодинаково."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdb0642-854b-4677-8260-49af1bbf811c",
   "metadata": {},
   "source": [
    "### Time Series Validation\n",
    "\n",
    "**Описание:**\n",
    "Специальные методы валидации для временных рядов, учитывающие временную последовательность данных.\n",
    "\n",
    "**Методы:**\n",
    "- Rolling-origin update evaluation.\n",
    "- Walk-forward validation.\n",
    "  \n",
    "**Особенности:**\n",
    "- Данные не перемешиваются.\n",
    "- Модель обучается на прошлых данных и проверяется на будущих."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40969fb-21da-4efb-8fe1-6dd688e13b84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
